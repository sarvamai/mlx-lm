# Copyright Â© 2023-2024 Apple Inc.


from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple, Union

import numpy as np

import mlx.core as mx
import mlx.nn as nn


from .base import BaseModelArgs, create_attention_mask, scaled_dot_product_attention
from .switch_layers import SwitchGLU


@dataclass
class SarvamMoEModelOutputWithPast:
    last_hidden_state: mx.array = None
    past_key_values: Optional[List[mx.array]] = None
    hidden_states: Optional[Tuple[mx.array]] = None
    attentions: Optional[Tuple[mx.array]] = None
    router_logits: Optional[Tuple[mx.array]] = None


@dataclass
class SarvamMoECausalLMOutputWithPast:
    loss: Optional[mx.array] = None
    logits: Optional[mx.array] = None
    past_key_values: Optional[List[mx.array]] = None
    hidden_states: Optional[Tuple[mx.array]] = None
    attentions: Optional[Tuple[mx.array]] = None

    z_loss: Optional[mx.array] = None
    aux_loss: Optional[mx.array] = None
    router_logits: Optional[Tuple[mx.array]] = None









@dataclass
class ModelArgs(BaseModelArgs):
    model_type: str
    vocab_size: int
    hidden_size: int
    intermediate_size: int
    num_hidden_layers: int
    num_attention_heads: int
    num_key_value_heads: int
    hidden_act: str = "silu"
    use_qkv_bias: bool = False
    use_bias: bool = False
    rms_norm_eps: float = 1e-6
    tie_word_embeddings: bool = False
    embedding_dropout: float = 0.0
    attention_dropout: float = 0.0
    output_dropout: float = 0.0
    initializer_range: float = 0.02
    max_position_embeddings: int = 4096
    rope_theta: float = 10000.0
    use_cache: bool = True
    max_window_layers: int = 19
    rope_scaling: Optional[Dict[str, Union[float, str]]] = None
    pad_token_id: int = 0
    eos_token_id: int = 1
    num_experts: int = 128
    num_shared_experts: int = 1
    num_experts_per_tok: int = 6
    n_group: int = 1
    topk_group: int = 1
    moe_intermediate_size: int = 1024
    first_k_dense_replace: int = 1
    head_dim: int = 256
    output_router_logits: bool = False
    use_qk_norm: bool = True
    moe_router_enable_expert_bias: bool = True
    routed_scaling_factor: float = 2.5
    attn_implementation: str = "eager"
    partial_rotary_factor: float = 0.5

    def __post_init__(self):
        if self.head_dim is None:
            self.head_dim = self.hidden_size // self.num_attention_heads
        if self.rope_scaling:
            if not isinstance(self.rope_scaling, dict):
                self.rope_scaling = None


class SarvamMoERotaryEmbedding(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.max_position_embeddings = args.max_position_embeddings
        self.rope_theta = args.rope_theta
        
        # Calculate rope dimension
        # Note: head_dim is usually set in args if not from hidden/heads
        dim = args.head_dim or (args.hidden_size // args.num_attention_heads)
        
        inv_freq = 1.0 / (
            self.rope_theta
            ** (mx.arange(0, dim, 2, dtype=mx.float32) / dim)
        )
        self.inv_freq = inv_freq
        self.attention_scaling = 1.0

    def __call__(self, x: mx.array, position_ids: mx.array) -> Tuple[mx.array, mx.array]:
        # position_ids: (1, L) or (B, L)
        
        inv_freq_expanded = self.inv_freq[None, :, None] # (1, D/2, 1)
        position_ids_expanded = position_ids[:, None, :].astype(mx.float32) # (B, 1, L)
        
        # (1, D/2, 1) * (B, 1, L) -> (B, D/2, L)
        freqs = inv_freq_expanded * position_ids_expanded
        
        # Transpose to (B, L, D/2)
        freqs = freqs.transpose(0, 2, 1)
        
        # emb = cat(freqs, freqs) -> (B, L, D)
        emb = mx.concatenate((freqs, freqs), axis=-1)
        
        cos = mx.cos(emb) * self.attention_scaling
        sin = mx.sin(emb) * self.attention_scaling
        
        return cos, sin


class SarvamMoERMSNorm(nn.RMSNorm):
    def __init__(self, dims: int, eps: float = 1e-6):
        super().__init__(dims, eps)

# rotate_half logic
def rotate_half(x):
    D = x.shape[-1]
    x1 = x[..., : D // 2]
    x2 = x[..., D // 2 :]
    return mx.concatenate([-x2, x1], axis=-1)

def apply_rotary_pos_emb(q, k, cos, sin):
    # q, k: (B, H, L, D_full) - full head dimension
    # cos, sin: (B, L, D_full) - full head dimension from SarvamMoERotaryEmbedding
    
    # Reshape for broadcasting: (B, L, D) -> (B, 1, L, D)
    cos = cos[:, None, :, :]
    sin = sin[:, None, :, :]
    
    # Get rotary dimension from cos (matching reference behavior)
    rotary_dim = cos.shape[-1]
    
    # Split q and k into rotary and pass-through parts
    q_rot = q[..., :rotary_dim]
    q_pass = q[..., rotary_dim:]
    k_rot = k[..., :rotary_dim]
    k_pass = k[..., rotary_dim:]
        
    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)
    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)
    
    # Concatenate back
    q_embed = mx.concatenate([q_embed, q_pass], axis=-1)
    k_embed = mx.concatenate([k_embed, k_pass], axis=-1)
    
    return q_embed, k_embed






class SarvamMoEMLP(nn.Module):
    def __init__(self, args: ModelArgs, intermediate_size: int):
        super().__init__()
        self.gate_proj = nn.Linear(args.hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(args.hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, args.hidden_size, bias=False)

    def __call__(self, x: mx.array) -> mx.array:
        return self.down_proj(nn.silu(self.gate_proj(x)) * self.up_proj(x))

class SarvamMoEAttention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args

        dim = args.hidden_size
        self.n_heads = args.num_attention_heads
        self.n_kv_heads = args.num_key_value_heads
        self.head_dim = args.head_dim or (dim // self.n_heads)
        self.scale = self.head_dim**-0.5
        self.partial_rotary_factor = args.partial_rotary_factor

        # Merged QKV projection
        self.query_key_value = nn.Linear(
            dim,
            (self.n_heads + 2 * self.n_kv_heads) * self.head_dim,
            bias=args.use_qkv_bias,
        )

        if args.use_qk_norm:
            self.query_layernorm = SarvamMoERMSNorm(self.head_dim, eps=args.rms_norm_eps)
            self.key_layernorm = SarvamMoERMSNorm(self.head_dim, eps=args.rms_norm_eps)
        else:
            self.query_layernorm = None
            self.key_layernorm = None

        self.dense = nn.Linear(self.n_heads * self.head_dim, dim, bias=args.use_bias)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
        position_embeddings: Optional[Tuple[mx.array, mx.array]] = None,
    ) -> mx.array:
        B, L, _ = x.shape

        qkv = self.query_key_value(x)
        # Split Q, K, V
        q_size = self.n_heads * self.head_dim
        kv_size = self.n_kv_heads * self.head_dim
        
        queries, keys, values = mx.split(
            qkv, [q_size, q_size + kv_size], axis=-1
        )
        
        # Transpose immediately to (B, H, L, D) for easier processing with RoPE and SDPA
        queries = queries.reshape(B, L, self.n_heads, self.head_dim).transpose(0, 2, 1, 3)
        keys = keys.reshape(B, L, self.n_kv_heads, self.head_dim).transpose(0, 2, 1, 3)
        values = values.reshape(B, L, self.n_kv_heads, self.head_dim).transpose(0, 2, 1, 3)

        if self.query_layernorm is not None:
            queries = self.query_layernorm(queries)
            keys = self.key_layernorm(keys)

        # Apply RoPE using position_embeddings if provided
        if position_embeddings is not None:
             cos, sin = position_embeddings
             queries, keys = apply_rotary_pos_emb(queries, keys, cos, sin)

        if cache is not None:
            keys, values = cache.update_and_fetch(keys, values)



        # Output: (B, H, L, D)
        output = scaled_dot_product_attention(
            queries, keys, values, cache=cache, scale=self.scale, mask=mask
        )
        
        # Transpose back: (B, H, L, D) -> (B, L, H, D) -> (B, L, Hidden)
        output = output.transpose(0, 2, 1, 3).reshape(B, L, -1)
        return self.dense(output)



class SarvamMoEGate(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.top_k = args.num_experts_per_tok
        self.num_experts = args.num_experts
        self.n_group = args.n_group
        self.topk_group = args.topk_group
        self.routed_scaling_factor = args.routed_scaling_factor
        
        # Use direct parameter for weight to match checkpoint structure (nn.Parameter in Torch)
        # Shape: (num_experts, hidden_size)
        scale = args.hidden_size ** -0.5
        self.weight = mx.random.uniform(
            low=-scale, high=scale,
            shape=(args.num_experts, args.hidden_size)
        )
        
        if args.moe_router_enable_expert_bias:
            self.expert_bias = mx.zeros((args.num_experts,))
        else:
            self.expert_bias = None

    def _topk(self, x: mx.array, k: int):
        inds = mx.argpartition(x, kth=-k, axis=-1)[..., -k:]
        vals = mx.take_along_axis(x, inds, axis=-1)
        # Sort by score desc
        order = mx.argsort(vals, axis=-1)[..., ::-1]
        inds = mx.take_along_axis(inds, order, axis=-1)
        vals = mx.take_along_axis(vals, order, axis=-1)
        return inds, vals

    def group_limited_topk(self, scores: mx.array):
        if self.n_group == 1:
            return self._topk(scores, self.top_k)
        
        num_tokens, num_experts = scores.shape
        group_scores = scores.reshape(num_tokens, self.n_group, -1)
        # Sum of top 2 in each group
        # mx.topk returns values directly (unsorted, but sum doesn't care)
        top2_vals = mx.topk(group_scores, k=2, axis=-1)
        group_score_sums = top2_vals.sum(axis=-1)
        
        # Select topk groups
        # We need indices here, so we use _topk (which wraps argpartition+sort)
        group_idx, _ = self._topk(group_score_sums, k=self.topk_group)
        
        # Create mask
        # Reference: group_mask.scatter_(1, group_idx, 1)
        # MLX supports scatter via advanced indexing
        group_mask = mx.zeros((num_tokens, self.n_group), dtype=scores.dtype)
        # [B, Kg] indices
        batch_col = mx.arange(num_tokens)[:, None]
        group_mask[batch_col, group_idx] = 1 # Set selected groups to 1
        
        # Expand mask to experts: [B, n_group] -> [B, n_group, 1] -> broadcast
        # experts_per_group implied by reshape
        experts_per_group = num_experts // self.n_group
        
        # We can reshape mask to [B, num_experts] by repeating elements?
        # group_mask: [B, G]. We want [B, G * E_per_G] where each expert in group G gets mask[g]
        score_mask = mx.repeat(group_mask[:, :, None], experts_per_group, axis=2)
        score_mask = score_mask.reshape(num_tokens, num_experts)
        
        # Apply mask
        neg_inf = -1e9
        masked_scores = mx.where(score_mask > 0, scores, neg_inf)
        
        # Final global topk
        return self._topk(masked_scores, self.top_k)

    def __call__(self, x: mx.array):
        # x: [B, L, H], weight: [E, H]
        # logits: [B, L, E] = x @ weight.T
        logits = x @ self.weight.T
        scores = mx.sigmoid(logits)
        
        scores_for_routing = scores
        if self.expert_bias is not None:
             scores_for_routing = scores_for_routing + self.expert_bias
        
        # Reshape for routing logic which typically expects 2D [Tokens, Experts] 
        # (though our implementation handles broadcasting, group_limited_topk assumes 2D)
        
        B, L, E = scores_for_routing.shape
        scores_flat = scores_for_routing.reshape(-1, E)
        
        # Routing
        inds_flat, _ = self.group_limited_topk(scores_flat)
        
        # Reshape back to [B, L, k]
        inds = inds_flat.reshape(B, L, self.top_k)
        
        # Gather weights from original scores (sigmoid, no bias)
        gathered_scores = mx.take_along_axis(scores, inds, axis=-1)
        
        if self.top_k > 1:
            denom = gathered_scores.sum(axis=-1, keepdims=True) + 1e-20
            topk_weight = gathered_scores / denom
        else:
            topk_weight = gathered_scores
            
        topk_weight = topk_weight * self.routed_scaling_factor
        
        return inds, topk_weight, logits


class SarvamMoEExperts(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        # We use SwitchGLU for efficient MoE execution in MLX
        self.switch_mlp = SwitchGLU(
            args.hidden_size, 
            args.moe_intermediate_size, 
            args.num_experts, 
            bias=False
        )
        
    def __call__(self, x: mx.array, topk_inds: mx.array, topk_weights: mx.array) -> mx.array:
        # switch_mlp expects (x, indices) and returns expert outputs
        # We then need to weight them.
        
        # SwitchGLU in mlx_lm typically takes (x, indices). 
        # But wait, SwitchGLU returns the aggregated output if it handles the gathering?
        # Let's check SwitchGLU implementation via memory or assumption.
        # Usually: y = switch_mlp(x, inds) -> returns result for each token?
        # If it returns [B, L, TopK, Dim], we sum.
        # Checking previous sarvam_moe.py:
        # y = self.switch_mlp(x, inds)
        # y = (y * scores[..., None]).sum(axis=-2)
        
        y = self.switch_mlp(x, topk_inds)
        y = (y * topk_weights[..., None]).sum(axis=-2)
        return y


class SarvamMoESparseMoeBlock(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.gate = SarvamMoEGate(args)
        self.experts = SarvamMoEExperts(args)
        
        if args.num_shared_experts > 0:
            shared_inter_size = args.moe_intermediate_size * args.num_shared_experts
            self.shared_experts = SarvamMoEMLP(args, shared_inter_size)
        else:
            self.shared_experts = None

    def __call__(self, x: mx.array) -> mx.array:
        identity = x
        topk_inds, topk_weights, router_logits = self.gate(x)
        
        y = self.experts(x, topk_inds, topk_weights)
        
        if self.shared_experts is not None:
             y = y + self.shared_experts(identity)
             
        return y, router_logits


class SarvamMoEDecoderLayer(nn.Module):
    def __init__(self, args: ModelArgs, layer_idx: int):
        super().__init__()
        self.hidden_size = args.hidden_size
        self.attention = SarvamMoEAttention(args) # Renamed to match reference and checkpoint

        self.input_layernorm = SarvamMoERMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.post_attention_layernorm = SarvamMoERMSNorm(args.hidden_size, eps=args.rms_norm_eps)

        first_k_dense = args.first_k_dense_replace
        
        # Condition from reference: (config.num_experts is not None and layer_idx >= config.first_k_dense_replace)
        is_moe = (args.num_experts > 0) and (layer_idx >= first_k_dense)
        
        if is_moe:
            self.mlp = SarvamMoESparseMoeBlock(args)
        else:
            self.mlp = SarvamMoEMLP(args, args.intermediate_size)

    def __call__(
        self,
        x: mx.array,
        mask: Optional[mx.array] = None,
        cache: Optional[Any] = None,
        position_embeddings: Optional[Tuple[mx.array, mx.array]] = None,
        output_attentions: bool = False,
        output_router_logits: bool = False,
        **kwargs,
    ) -> mx.array:
        r = self.attention(self.input_layernorm(x), mask, cache, position_embeddings)
        h = x + r
        
        r = self.mlp(self.post_attention_layernorm(h))
        
        router_logits = None
        if isinstance(r, tuple):
            r, rl = r
            if output_router_logits:
                router_logits = rl
            
        out = h + r
        return out, router_logits


class SarvamMoEModel(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.vocab_size = args.vocab_size
        self.embed_tokens = nn.Embedding(args.vocab_size, args.hidden_size)
        self.layers = [
            SarvamMoEDecoderLayer(args, i) for i in range(args.num_hidden_layers)
        ]
        self.norm = SarvamMoERMSNorm(args.hidden_size, eps=args.rms_norm_eps)
        self.rotary_emb = SarvamMoERotaryEmbedding(args)

    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        input_embeddings: Optional[mx.array] = None,
        output_router_logits: bool = False,
    ):
        if input_embeddings is not None:
            h = input_embeddings
        else:
            h = self.embed_tokens(inputs)

        if cache is None:
            cache = [None] * len(self.layers)

        mask = None
        mask = None
        if h.shape[1] > 1:
            mask = create_attention_mask(h, cache)

        all_router_logits = [] if output_router_logits else None

        # position_ids: (1, L)
        start = 0
        if cache and cache[0] is not None:
             start = cache[0].offset
        L = h.shape[1]
        position_ids = mx.arange(start, start + L).reshape(1, -1)
        
        cos, sin = self.rotary_emb(h, position_ids)
        position_embeddings = (cos, sin)

        for layer, c in zip(self.layers, cache):
            h, router_logits = layer(
                h, 
                mask, 
                c, 
                position_embeddings, 
                output_router_logits=output_router_logits
            )
            if output_router_logits and router_logits is not None:
                all_router_logits.append(router_logits)

        out = self.norm(h)
        
        if output_router_logits:
             return SarvamMoEModelOutputWithPast(
                 last_hidden_state=out,
                 past_key_values=cache,
                 router_logits=tuple(all_router_logits) if all_router_logits else None
             )

        return out


class SarvamMoEForCausalLM(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.args = args
        self.model = SarvamMoEModel(args)
        self.vocab_size = args.vocab_size
        if not args.tie_word_embeddings:
            self.lm_head = nn.Linear(args.hidden_size, args.vocab_size, bias=False)
    
    def __call__(
        self,
        inputs: mx.array,
        cache=None,
        input_embeddings: Optional[mx.array] = None,
        output_router_logits: bool = False,
    ):
        outputs = self.model(inputs, cache, input_embeddings, output_router_logits=output_router_logits)
        
        if output_router_logits and isinstance(outputs, SarvamMoEModelOutputWithPast):
            hidden_state = outputs.last_hidden_state
            
            if self.args.tie_word_embeddings:
                logits = self.model.embed_tokens.as_linear(hidden_state)
            else:
                logits = self.lm_head(hidden_state)
            
            return SarvamMoECausalLMOutputWithPast(
                logits=logits,
                past_key_values=outputs.past_key_values,
                router_logits=outputs.router_logits,
                hidden_states=outputs.hidden_states,
                attentions=outputs.attentions,
            )
        
        # Simple tensor output
        if self.args.tie_word_embeddings:
            out = self.model.embed_tokens.as_linear(outputs)
        else:
            out = self.lm_head(outputs)
        return out


class Model(SarvamMoEForCausalLM):
    def __init__(self, args: ModelArgs):
        super().__init__(args)
        self.model_type = args.model_type

    @property
    def layers(self):
        return self.model.layers

    @property
    def head_dim(self):
        return self.args.head_dim or (self.args.hidden_size // self.args.num_attention_heads)

    @property
    def n_kv_heads(self):
        return self.args.num_key_value_heads



    def sanitize(self, weights):
        # Remove unused keys (like FP8 scales) to avoid strict load errors
        keys_to_remove = [k for k in weights.keys() if "input_scale" in k or "weight_scale" in k]
        for k in keys_to_remove:
            weights.pop(k, None)

        # Add inv_freq if missing (it's often not in checkpoints but is a parameter in MLX RoPE)
        if "model.rotary_emb.inv_freq" not in weights:
            weights["model.rotary_emb.inv_freq"] = self.model.rotary_emb.inv_freq

        # Remove unused weights
        # Reference uses 'word_embeddings' but we use 'embed_tokens' in MLX standard
        # So we might need to map 'model.word_embeddings.weight' -> 'model.embed_tokens.weight'
        if "model.word_embeddings.weight" in weights:
            weights["model.embed_tokens.weight"] = weights.pop("model.word_embeddings.weight")

        if self.args.tie_word_embeddings:
            weights.pop("lm_head.weight", None)

        for layer_idx in range(self.args.num_hidden_layers):
            prefix = f"model.layers.{layer_idx}"
            mlp_prefix = f"{prefix}.mlp"

            # Check if it is MoE or Dense
            # If Dense: gate_proj, up_proj, down_proj
            # If MoE: experts, gate, shared_experts
            
            # Shared Experts mapping
            # Reference: layers.*.mlp.shared_experts.gate_proj etc.
            # Our model: Same structure.
            
            # Experts mapping
            # We need to stack expert weights for SwitchGLU
            # experts.{e}.gate_proj.weight -> stack -> switch_mlp.gate_proj.weight
            
            # Check for expert weights
            if f"{mlp_prefix}.experts.0.gate_proj.weight" in weights:
                for n in ["gate_proj", "up_proj", "down_proj"]:
                    # Collect from experts 0..N
                    w_list = []
                    for e in range(self.args.num_experts):
                        key = f"{mlp_prefix}.experts.{e}.{n}.weight"
                        if key in weights:
                            w_list.append(weights.pop(key))
                    
                    if w_list:
                        stacked = mx.stack(w_list)
                        weights[f"{mlp_prefix}.experts.switch_mlp.{n}.weight"] = stacked

        return weights


